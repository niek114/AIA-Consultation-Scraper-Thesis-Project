{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92f5d41b-d1a0-4c47-8f72-1fc6d6436d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "\n",
    "print(LAW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de1643dd-ffa2-4503-80dc-856b9ef2f623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting: proposal_2021\n",
      "Text length: 315642\n",
      "Articles found: 254\n",
      "Extracting: ep_2024\n",
      "Text length: 592676\n",
      "Articles found: 522\n",
      "Extracting: final_2024\n",
      "Text length: 599918\n",
      "Articles found: 528\n",
      "DONE. Saved to: C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\\ai_act_articles_clean.csv\n",
      "Rows: 1304\n",
      "          source article_number  \\\n",
      "0  proposal_2021            225   \n",
      "1  proposal_2021            114   \n",
      "2  proposal_2021             16   \n",
      "3  proposal_2021            288   \n",
      "4  proposal_2021              1   \n",
      "\n",
      "                                      article_header  \\\n",
      "0                             Article 225 TFEU, this   \n",
      "1                   Article 114 of the Treaty on the   \n",
      "2                            Article 16 of the TFEU.   \n",
      "3  Article 288 TFEU, will reduce legal fragmentat...   \n",
      "4  Article 1), respect for private life and prote...   \n",
      "\n",
      "                                        article_text  \n",
      "0  3 \\nEuropean Council, Special meeting of the E...  \n",
      "1  Functioning of the European Union (TFEU), whic...  \n",
      "2  2.2. \\nSubsidiarity (for non-exclusive compete...  \n",
      "3  facilitate the development of a single market ...  \n",
      "4                                   discrimination (  \n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# FIXED PATHS (original plan)\n",
    "# ---------------------------------------------------\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "\n",
    "PDF1 = LAW / \"ai_act_2021_proposal.pdf\"\n",
    "PDF2 = LAW / \"ai_act_2024_ep_position.pdf\"\n",
    "PDF3 = LAW / \"ai_act_2024_final_oj.pdf\"\n",
    "\n",
    "PDFS = {\n",
    "    \"proposal_2021\": PDF1,\n",
    "    \"ep_2024\": PDF2,\n",
    "    \"final_2024\": PDF3\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# STEP 1 — PDF → Raw Text\n",
    "# ---------------------------------------------------\n",
    "def pdf_to_text(path: Path) -> str:\n",
    "    doc = fitz.open(path)\n",
    "    pages = []\n",
    "    for page in doc:\n",
    "        pages.append(page.get_text(\"text\"))\n",
    "    doc.close()\n",
    "    return \"\\n\".join(pages)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# STEP 2 — Split Into Articles\n",
    "# ---------------------------------------------------\n",
    "ARTICLE_RE = re.compile(\n",
    "    r\"(Article\\s+(\\d+)[^\\n]*)([\\s\\S]*?)(?=(Article\\s+\\d+)|\\Z)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "def split_by_article(txt: str, source: str):\n",
    "    out = []\n",
    "    for m in ARTICLE_RE.finditer(txt):\n",
    "        out.append({\n",
    "            \"source\": source,\n",
    "            \"article_number\": m.group(2),\n",
    "            \"article_header\": (m.group(1) or \"\").strip(),\n",
    "            \"article_text\": (m.group(3) or \"\").strip()\n",
    "        })\n",
    "    return out\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# STEP 3 — Remove Characters Illegal for Excel\n",
    "# ---------------------------------------------------\n",
    "def strip_illegal(s):\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return \"\".join(ch for ch in s if ch in (\"\\n\", \"\\t\") or ord(ch) >= 32)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# MAIN RUN — EXACT ORIGINAL PLAN\n",
    "# ---------------------------------------------------\n",
    "rows = []\n",
    "\n",
    "for label, pdf in PDFS.items():\n",
    "    print(f\"Extracting: {label}\")\n",
    "    raw = pdf_to_text(pdf)\n",
    "    print(f\"Text length: {len(raw)}\")\n",
    "    parts = split_by_article(raw, label)\n",
    "    print(f\"Articles found: {len(parts)}\")\n",
    "    rows.extend(parts)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "for col in [\"article_header\", \"article_text\"]:\n",
    "    df[col] = df[col].astype(str).apply(strip_illegal)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# STEP 4 — Save One Clean CSV With Semicolon\n",
    "# ---------------------------------------------------\n",
    "OUT = LAW / \"ai_act_articles_clean.csv\"\n",
    "df.to_csv(OUT, sep=\";\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"DONE. Saved to:\", OUT)\n",
    "print(\"Rows:\", len(df))\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5270dec-d1e8-494b-a006-ebf8d3d4432e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ai_act_2021_proposal.pdf\n",
      "Processing: ai_act_2024_ep_position.pdf\n",
      "Processing: ai_act_2024_final_oj.pdf\n",
      "Saved: C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\\ai_act_articles.xlsx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp EliteBook\\AppData\\Local\\Temp\\ipykernel_17692\\1119991267.py:45: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(clean_text_for_excel)\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "OUT = LAW / \"ai_act_articles.xlsx\"\n",
    "\n",
    "def clean_text_for_excel(text):\n",
    "    if text is None:\n",
    "        return None\n",
    "    text = str(text)\n",
    "    return re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \"\", text)\n",
    "\n",
    "def extract_articles(pdf_path, source_name):\n",
    "    print(\"Processing:\", pdf_path.name)\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    raw_text = \" \".join(page.get_text() for page in doc)\n",
    "    raw_text = raw_text.replace(\"\\r\", \" \").replace(\"\\n\", \" \")\n",
    "    parts = re.split(r\"(Article\\s+\\d+)\", raw_text, flags=re.IGNORECASE)\n",
    "\n",
    "    articles = []\n",
    "    for i in range(1, len(parts), 2):\n",
    "        number = parts[i].strip()\n",
    "        text = parts[i+1].strip() if i+1 < len(parts) else \"\"\n",
    "        articles.append({\n",
    "            \"source\": source_name,\n",
    "            \"article_number\": number,\n",
    "            \"article_text\": text\n",
    "        })\n",
    "    return articles\n",
    "\n",
    "pdf_files = {\n",
    "    \"Proposal_2021\": LAW / \"ai_act_2021_proposal.pdf\",\n",
    "    \"EP_Position_2024\": LAW / \"ai_act_2024_ep_position.pdf\",\n",
    "    \"Final_OJ_2024\": LAW / \"ai_act_2024_final_oj.pdf\"\n",
    "}\n",
    "\n",
    "records = []\n",
    "for label, pdf_path in pdf_files.items():\n",
    "    records.extend(extract_articles(pdf_path, label))\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "df = df.applymap(clean_text_for_excel)\n",
    "df.to_excel(OUT, index=False)\n",
    "print(\"Saved:\", OUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8efb89b0-8a38-41f1-81e2-7e3a4c044bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: ai_act_2021_proposal.pdf\n",
      "Processing: ai_act_2024_ep_position.pdf\n",
      "Processing: ai_act_2024_final_oj.pdf\n",
      "source\n",
      "EP_Position_2024    113\n",
      "Final_OJ_2024       113\n",
      "Proposal_2021        85\n",
      "Name: article_number, dtype: int64\n",
      "               source article_number article_title  \\\n",
      "89   EP_Position_2024              1          None   \n",
      "99   EP_Position_2024             10          None   \n",
      "194  EP_Position_2024            100          None   \n",
      "195  EP_Position_2024            101          None   \n",
      "196  EP_Position_2024            102          None   \n",
      "\n",
      "                                          article_text  \n",
      "89   Subject matter`\\n1.\\nThe purpose of this Regul...  \n",
      "99   Data and data governance\\n1.\\nHigh-risk AI sys...  \n",
      "194  Administrative fines on Union institutions, bo...  \n",
      "195  Fines for providers of general-purpose AI mode...  \n",
      "196  Amendment to Regulation (EC) No 300/2008\\nIn A...  \n",
      "Saved: C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\\ai_act_articles_clean.xlsx\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Paths\n",
    "# -------------------------------------------------------------------\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "\n",
    "pdf_files = {\n",
    "    \"Proposal_2021\": LAW / \"ai_act_2021_proposal.pdf\",\n",
    "    \"EP_Position_2024\": LAW / \"ai_act_2024_ep_position.pdf\",\n",
    "    \"Final_OJ_2024\": LAW / \"ai_act_2024_final_oj.pdf\",\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Helpers\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def strip_illegal(s: str) -> str:\n",
    "    \"\"\"Remove characters Excel / openpyxl rejects.\"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return s\n",
    "    return re.sub(r\"[\\x00-\\x08\\x0B-\\x0C\\x0E-\\x1F]\", \"\", s)\n",
    "\n",
    "def extract_articles_strict(pdf_path: Path, source_name: str):\n",
    "    \"\"\"\n",
    "    Extract articles from a single AI Act PDF.\n",
    "\n",
    "    Logic:\n",
    "    - read page by page\n",
    "    - normalize newlines and simple hyphenation\n",
    "    - a line is an article heading iff:\n",
    "        * it starts with 'Article <number>'\n",
    "        * AND the first non-space character after the number is\n",
    "          uppercase or a digit (so 'Article 62 shall apply...' is ignored)\n",
    "    - everything until the next heading is that article's text\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {pdf_path.name}\")\n",
    "\n",
    "    doc = fitz.open(str(pdf_path))\n",
    "    lines = []\n",
    "\n",
    "    for page in doc:\n",
    "        text = page.get_text(\"text\")\n",
    "        text = text.replace(\"\\r\", \"\\n\")\n",
    "        # simple de-hyphenation\n",
    "        text = re.sub(r\"(\\w+)-\\n(\\w+)\", r\"\\1\\2\", text)\n",
    "        for ln in text.splitlines():\n",
    "            lines.append(ln.rstrip())\n",
    "\n",
    "    records = []\n",
    "    current_art = None\n",
    "    current_title = None\n",
    "    buffer = []\n",
    "\n",
    "    # heading: \"Article 5\", \"Article 5 Subject matter\", \"Article 5 - Subject matter\", etc.\n",
    "    art_re = re.compile(r\"^\\s*Article\\s+(\\d+[A-Z]?)\\b(?:\\s*(.*))?$\", re.IGNORECASE)\n",
    "\n",
    "    for ln in lines:\n",
    "        m = art_re.match(ln)\n",
    "        if m:\n",
    "            tail = (m.group(2) or \"\").strip(\" .–-\")\n",
    "\n",
    "            # Guard: ignore cross-references like \"Article 62 shall apply …\"\n",
    "            if tail and not (tail[0].isupper() or tail[0].isdigit()):\n",
    "                if current_art is not None:\n",
    "                    buffer.append(ln)\n",
    "                continue\n",
    "\n",
    "            # Flush previous article\n",
    "            if current_art is not None:\n",
    "                text_block = \"\\n\".join(buffer).strip()\n",
    "                if text_block:\n",
    "                    records.append(\n",
    "                        {\n",
    "                            \"source\": source_name,\n",
    "                            \"article_number\": current_art,\n",
    "                            \"article_title\": strip_illegal(current_title),\n",
    "                            \"article_text\": strip_illegal(text_block),\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            # Start new article\n",
    "            current_art = m.group(1).strip()\n",
    "            current_title = tail if tail else None\n",
    "            buffer = []\n",
    "        else:\n",
    "            if current_art is not None:\n",
    "                buffer.append(ln)\n",
    "\n",
    "    # Flush last article\n",
    "    if current_art is not None and buffer:\n",
    "        text_block = \"\\n\".join(buffer).strip()\n",
    "        if text_block:\n",
    "            records.append(\n",
    "                {\n",
    "                    \"source\": source_name,\n",
    "                    \"article_number\": current_art,\n",
    "                    \"article_title\": strip_illegal(current_title),\n",
    "                    \"article_text\": strip_illegal(text_block),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return records\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Run extraction for all three PDFs\n",
    "# -------------------------------------------------------------------\n",
    "all_records = []\n",
    "for label, pdf_path in pdf_files.items():\n",
    "    all_records.extend(extract_articles_strict(pdf_path, label))\n",
    "\n",
    "articles_df = pd.DataFrame(all_records)\n",
    "\n",
    "# Optional: sort for sanity\n",
    "articles_df = articles_df.sort_values([\"source\", \"article_number\"], key=lambda s: s.astype(str))\n",
    "\n",
    "print(articles_df.groupby(\"source\")[\"article_number\"].nunique())\n",
    "print(articles_df.head())\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# Save to Excel\n",
    "# -------------------------------------------------------------------\n",
    "OUT_XLSX = LAW / \"ai_act_articles_clean.xlsx\"\n",
    "articles_df.to_excel(OUT_XLSX, index=False)\n",
    "print(\"Saved:\", OUT_XLSX)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cb79344-e0fa-48c6-9ab1-b07a0d78a2d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting from: ai_act_2021_proposal.pdf\n",
      "\n",
      "Extracting from: ai_act_2024_ep_position.pdf\n",
      "\n",
      "Extracting from: ai_act_2024_final_oj.pdf\n",
      "\n",
      "Saved to: C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\\ai_act_articles_clean.xlsx\n",
      "Rows extracted: 375\n",
      "source\n",
      "ep_position_2024    113\n",
      "final_oj_2024       113\n",
      "proposal_2021        85\n",
      "Name: article_number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Paths\n",
    "# ---------------------------------------------------------------------\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "OUT = LAW / \"ai_act_articles_clean.xlsx\"\n",
    "\n",
    "pdf_files = {\n",
    "    \"proposal_2021\": LAW / \"ai_act_2021_proposal.pdf\",\n",
    "    \"ep_position_2024\": LAW / \"ai_act_2024_ep_position.pdf\",\n",
    "    \"final_oj_2024\": LAW / \"ai_act_2024_final_oj.pdf\",\n",
    "}\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Utility: Clean text to avoid Excel corruption\n",
    "# ---------------------------------------------------------------------\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    # Remove illegal Excel characters\n",
    "    s = re.sub(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\", \"\", s)\n",
    "    # Normalize whitespace\n",
    "    s = re.sub(r\"[ \\t]+\", \" \", s)\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
    "    # Fix hyphenation across line breaks\n",
    "    s = re.sub(r\"-\\s*\\n\\s*\", \"\", s)\n",
    "    return s.strip()\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Heading detector (bulletproof)\n",
    "# ---------------------------------------------------------------------\n",
    "def is_article_heading(line: str) -> re.Match:\n",
    "    \"\"\"\n",
    "    TRUE heading examples:\n",
    "        Article 5\n",
    "        Article 5 Definitions\n",
    "        Article 5 – Definitions\n",
    "        Article 5: Definitions\n",
    "\n",
    "    FALSE headings (should be ignored):\n",
    "        In accordance with Article 5…\n",
    "        Article 14 shall apply…\n",
    "        Article 3 may be used…\n",
    "\n",
    "    Rule:\n",
    "    - Must start at line beginning\n",
    "    - Title must start with capital letter OR be empty\n",
    "    - Next word must NOT be: shall, may, does, is, are, must, should\n",
    "    \"\"\"\n",
    "\n",
    "    pattern = r\"^(Article)\\s+(\\d+)\\s*(.*)$\"\n",
    "    m = re.match(pattern, line.strip())\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    title = m.group(3).strip()\n",
    "\n",
    "    # If title begins with lowercase verb → reject\n",
    "    forbidden_starts = (\"shall\", \"may\", \"does\", \"is\", \"are\", \"must\", \"should\", \"be\", \"has\", \"have\")\n",
    "    if title:\n",
    "        first = title.split()[0].lower()\n",
    "        if first in forbidden_starts:\n",
    "            return None\n",
    "\n",
    "        # If starts with lowercase letter → reject\n",
    "        if first[0].islower():\n",
    "            return None\n",
    "\n",
    "    return m\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Extraction function\n",
    "# ---------------------------------------------------------------------\n",
    "def extract_articles_from_pdf(pdf_path: Path, source_name: str):\n",
    "    print(f\"\\nExtracting from: {pdf_path.name}\")\n",
    "    doc = fitz.open(pdf_path)\n",
    "\n",
    "    all_text = \"\"\n",
    "    for page in doc:\n",
    "        all_text += page.get_text()\n",
    "\n",
    "    raw_lines = all_text.split(\"\\n\")\n",
    "\n",
    "    articles = []\n",
    "    current_article = None\n",
    "    current_title = \"\"\n",
    "    body_lines = []\n",
    "\n",
    "    for line in raw_lines:\n",
    "\n",
    "        m = is_article_heading(line)\n",
    "        if m:\n",
    "            # Save previous article before opening new one\n",
    "            if current_article is not None:\n",
    "                articles.append({\n",
    "                    \"source\": source_name,\n",
    "                    \"article_number\": current_article,\n",
    "                    \"article_title\": current_title,\n",
    "                    \"article_text\": clean_text(\"\\n\".join(body_lines))\n",
    "                })\n",
    "\n",
    "            current_article = m.group(2)\n",
    "            current_title = m.group(3).strip()\n",
    "            body_lines = []\n",
    "            continue\n",
    "\n",
    "        # Otherwise: normal body text\n",
    "        if current_article is not None:\n",
    "            body_lines.append(line)\n",
    "\n",
    "    # Close final article\n",
    "    if current_article is not None:\n",
    "        articles.append({\n",
    "            \"source\": source_name,\n",
    "            \"article_number\": current_article,\n",
    "            \"article_title\": current_title,\n",
    "            \"article_text\": clean_text(\"\\n\".join(body_lines))\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# RUN EXTRACTION\n",
    "# ---------------------------------------------------------------------\n",
    "records = []\n",
    "for label, pdf_path in pdf_files.items():\n",
    "    records.extend(extract_articles_from_pdf(pdf_path, label))\n",
    "\n",
    "articles_df = pd.DataFrame(records)\n",
    "\n",
    "# Safety cleaning for Excel\n",
    "articles_df[\"article_text\"] = articles_df[\"article_text\"].apply(clean_text)\n",
    "articles_df[\"article_title\"] = articles_df[\"article_title\"].apply(clean_text)\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# SAVE AS EXCEL\n",
    "# ---------------------------------------------------------------------\n",
    "articles_df.to_excel(OUT, index=False)\n",
    "print(\"\\nSaved to:\", OUT)\n",
    "print(\"Rows extracted:\", len(articles_df))\n",
    "\n",
    "# Quick check\n",
    "print(articles_df.groupby(\"source\")[\"article_number\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0aeb967-02d0-467e-8248-6f12406ea502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\\ai_act_articles_collapsed.xlsx\n",
      "Rows before: 375 → after collapse: 311\n",
      "source\n",
      "ep_position_2024    113\n",
      "final_oj_2024       113\n",
      "proposal_2021        85\n",
      "Name: article_number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "IN_XLSX = LAW / \"ai_act_articles_clean.xlsx\"\n",
    "OUT_XLSX = LAW / \"ai_act_articles_collapsed.xlsx\"\n",
    "\n",
    "# Load\n",
    "df = pd.read_excel(IN_XLSX, dtype=str)\n",
    "\n",
    "# Ensure no NaNs and compute text length\n",
    "df[\"article_text\"] = df[\"article_text\"].fillna(\"\")\n",
    "df[\"article_title\"] = df[\"article_title\"].fillna(\"\")\n",
    "df[\"text_len\"] = df[\"article_text\"].str.len()\n",
    "\n",
    "# Sort so the longest text per (source, article_number) comes first\n",
    "df_sorted = df.sort_values([\"source\", \"article_number\", \"text_len\"], ascending=[True, True, False])\n",
    "\n",
    "# Collapse: take the first (i.e. longest) row per (source, article_number)\n",
    "collapsed = (\n",
    "    df_sorted\n",
    "    .groupby([\"source\", \"article_number\"], as_index=False)\n",
    "    .agg({\n",
    "        \"article_title\": \"first\",\n",
    "        \"article_text\": \"first\"\n",
    "    })\n",
    ")\n",
    "\n",
    "# Optional: sort for sanity\n",
    "collapsed = collapsed.sort_values([\"source\", \"article_number\"], key=lambda s: s.astype(str))\n",
    "\n",
    "# Save\n",
    "collapsed.to_excel(OUT_XLSX, index=False)\n",
    "print(\"Saved:\", OUT_XLSX)\n",
    "print(\"Rows before:\", len(df), \"→ after collapse:\", len(collapsed))\n",
    "print(collapsed.groupby(\"source\")[\"article_number\"].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00a89c47-276a-43d5-83d9-24d1e906f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_selective(text):\n",
    "    text_l = text.lower()\n",
    "    if \"risk\" in text_l: return \"Risk Classification\"\n",
    "    if \"conform\" in text_l or \"technical\" in text_l: return \"Technical Requirements\"\n",
    "    if \"oversight\" in text_l or \"human\" in text_l: return \"Human Oversight\"\n",
    "    if \"authority\" in text_l or \"board\" in text_l: return \"Governance & Oversight\"\n",
    "    if \"fundamental\" in text_l or \"rights\" in text_l: return \"Fundamental Rights & Ethics\"\n",
    "    if \"transparen\" in text_l or \"information\" in text_l: return \"Transparency & Information\"\n",
    "    if \"provider\" in text_l or \"deployer\" in text_l: return \"Market Access & Obligations\"\n",
    "    if \"sandbox\" in text_l or \"innovation\" in text_l: return \"Innovation & Exceptions\"\n",
    "    if \"penalt\" in text_l or \"enforcement\" in text_l: return \"Enforcement & Penalties\"\n",
    "    return \"Scope & Definitions\"\n",
    "\n",
    "df[\"selective_code\"] = df[\"article_text\"].fillna(\"\").apply(guess_selective)\n",
    "df.to_excel(OUTFILE, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89db2c57-1974-4c94-ad00-080b19b58442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding template saved to: C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\\data\\law\\ai_act_articles_coding_template.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE = Path(r\"C:\\Users\\hp EliteBook\\Desktop\\ai-act-consultation-analysis\")\n",
    "LAW = BASE / \"data\" / \"law\"\n",
    "INFILE = LAW / \"ai_act_articles_collapsed.xlsx\"\n",
    "OUTFILE = LAW / \"ai_act_articles_coding_template.xlsx\"\n",
    "\n",
    "df = pd.read_excel(INFILE, dtype=str)\n",
    "\n",
    "# Add coding columns\n",
    "df[\"selective_code\"] = \"\"\n",
    "df[\"axial_problem\"] = \"\"\n",
    "df[\"axial_mechanism\"] = \"\"\n",
    "df[\"axial_actor\"] = \"\"\n",
    "df[\"axial_outcome\"] = \"\"\n",
    "df[\"notes\"] = \"\"\n",
    "\n",
    "df.to_excel(OUTFILE, index=False)\n",
    "print(\"Coding template saved to:\", OUTFILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4162cf03-dd9e-4f2f-8735-afbbaf79a789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
